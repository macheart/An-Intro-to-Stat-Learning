---
title: "chpt6 Applied Questions"
author: "Prisma Erika Lopez Jimenez"
date: "1/14/2020"
output: html_document
---

```{r Load your Workspace}
library(leaps)
library(ISLR)
library(glmnet)
```

```{r}
# Applied Question 8
# In this exercise, we will generate simulated data, and will then use this data to perform best subset selection
# (a) Use the rnorm() function to generate a predictor X of length n=100, as well as a noise vector epsilon of length n=100.

x=rnorm(100)
epsilon=rnorm(100) # 

# (b) Generate a response vector Y of length n=100 according to the model: Y=B0+B1X+B2X^2+B3X^3 + epsilon
# Where Bn are constants of your choice
y.f=function(x,epsilon){
Y=0.56+0.49*x+0.23*x^2+0.57*x^3+epsilon 
return(Y)
}
Y=y.f(x,epsilon) # 

# (c) Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X, X^2,....,X^10. What is the best model obtained according to Cp,BIC, and adjusted R^2? Show some plots to provide evidence for your answer and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y. 

coef(mod,9) # coefficients 
#### non-random

set.seed(1)
#train=sample(c(TRUE,FALSE),nrow(Hitters),rep=TRUE)
#test=(!train)

fit.best=regsubsets(Salary~.,data=Hitters,nvmax=19)
fit.best.sum=summary(fit.best)
names(fit.best.sum) # adjustment methods

par(mfrow=c(2,2))
plot(fit.best.sum$rss,xlab="Number of Variables",ylab="RSS",type="l") 
plot(fit.best.sum$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
which.max(fit.best.sum$adjr2) # max val for adjr2
points(11,fit.best.sum$adjr2[11],col="red",cex=2,pch=20)

plot(fit.best.sum$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(fit.best.sum$cp) # min val for cp
points(10,fit.best.sum$cp[10],col="red",cex=2,pch=20)
which.min(fit.best.sum$bic) # min val for bic
plot(fit.best.sum$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(6,fit.best.sum$bic[6],col="red",cex=2,pch=20)

plot(fit.best,scale="r2")
plot(fit.best,scale="adjr2") # 10 associ. models
plot(fit.best,scale="Cp") # 10 associ. models
plot(fit.best,scale="bic") # 6 associ. models

coef(fit.best,6) # coeff. for variables
coef

# (d) Apply forward step-wise selection and also using backward stepwise selection. How does your answer compare to the results above?
#### forward step-wise selection
fit.for=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(fit.for)


#### backwards step-wise selection
fit.bw=regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")

#### comparisons
# each list is diff:
coef(fit.best,7)
coef(fit.for,7)
coef(fit.bw,7)
# same:
coef(fit.best,2) 
coef(fit.for,2)

# (e) Fit a lasso model to the simulated data, again using X, X^2,...,X^10 as predictors. Use cross-validation to select the optimal value of lambda. Create plots of the cross-validation error as a function of lambda. Report the resulting coefficient estimates, and discuss the results obtained.
set.seed(1)
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
grid=10^seq(10,-2,length=100)


lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)

# (f) Now generate a response vector Y according to the model:
# Y=B(not) +B(7)X^7+epsilon
# And perform best subset selection and the lasso, Discuss the results obtained.
 

```

```{r}

```

